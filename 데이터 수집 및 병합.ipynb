{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> ## InfluxDB 연결 설정\n",
    "InfluxDB 연결 정보 설정 및 장비 맵핑"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:50.813465Z",
     "start_time": "2024-05-05T06:31:43.284058Z"
    }
   },
   "source": [
    "%pip install influxdb_client"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: influxdb_client in /usr/local/anaconda3/lib/python3.11/site-packages (1.42.0)\r\n",
      "Requirement already satisfied: reactivex>=4.0.4 in /usr/local/anaconda3/lib/python3.11/site-packages (from influxdb_client) (4.0.4)\r\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/anaconda3/lib/python3.11/site-packages (from influxdb_client) (2024.2.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/anaconda3/lib/python3.11/site-packages (from influxdb_client) (2.8.2)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/anaconda3/lib/python3.11/site-packages (from influxdb_client) (68.2.2)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/anaconda3/lib/python3.11/site-packages (from influxdb_client) (2.0.7)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.5.3->influxdb_client) (1.16.0)\r\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /usr/local/anaconda3/lib/python3.11/site-packages (from reactivex>=4.0.4->influxdb_client) (4.9.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:50.834807Z",
     "start_time": "2024-05-05T06:31:50.822771Z"
    }
   },
   "source": [
    "import os\n",
    "import pytz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime, timedelta\n",
    "from influxdb_client import InfluxDBClient"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:50.846441Z",
     "start_time": "2024-05-05T06:31:50.839093Z"
    }
   },
   "source": [
    "# InfluxDB 설정 정보\n",
    "url = \"http://133.186.144.22:8086\"\n",
    "token = \"r3Ecro-rJQ82UpyNScnHXYDZ3KaE45AzweCXz6QIv2jeo7eOP4hL4-A9uKvkAVQDg_xavWorGUGZn7MI_sPCwg==\"\n",
    "org = \"smoothing\"\n",
    "\n",
    "# Device ID와 위치를 매핑\n",
    "location_mapping = {\n",
    "    '24e124126d152919': 'indoor',\n",
    "    '24e124126d152969': 'bottom_right_corner',\n",
    "    '24e124128c067999': 'indoor',\n",
    "    '24e124785c389818': 'bottom_left_corner',\n",
    "    '24e124785c421885': 'top_right_corner'\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 시간대 설정\n",
    "InfluxDB의 조회 시간이 국제 표준시 UTC로 되어 있기 때문에 한국 표준시로 변경을 해야한다. \n",
    "\n",
    "따라서 아래 내용에 맞게 한국 표준시로 설정하였다."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:50.874244Z",
     "start_time": "2024-05-05T06:31:50.854940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 한국 시간대 설정\n",
    "korea_tz = pytz.timezone('Asia/Seoul')\n",
    "\n",
    "# 실행 시점의 날짜를 기준으로 전날의 날짜를 계산\n",
    "today_kst = datetime.now(korea_tz)\n",
    "yesterday_kst = today_kst - timedelta(days=1)\n",
    "\n",
    "# 전날의 시작과 끝 시간을 한국 시간대로 설정\n",
    "start_time_kst = korea_tz.localize(datetime(yesterday_kst.year, yesterday_kst.month, yesterday_kst.day, 0, 0, 0))\n",
    "end_time_kst = start_time_kst + timedelta(days=1)\n",
    "\n",
    "# UTC로 시간 변환\n",
    "start_time_utc = start_time_kst.astimezone(pytz.utc)\n",
    "end_time_utc = end_time_kst.astimezone(pytz.utc)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 함수 기능\n",
    "\n",
    "기능 구현에 따른 함수를 사용하기 위한 내용을 아래에 작성하였다.\n",
    "\n",
    "(구현이 완료되면 구현 마다 각 함수 내용 작성)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:50.922266Z",
     "start_time": "2024-05-05T06:31:50.877682Z"
    }
   },
   "source": [
    "def create_client(url, token, org):\n",
    "    \"\"\"\n",
    "    Influx DB 연결 Client를 생성합니다.\n",
    "    \n",
    "    :param url: InfluxDB 연결 주소\n",
    "    :param token: InfluxDB 토큰\n",
    "    :param org:  InfluxDB 조직\n",
    "    :return: InfluxDBClient\n",
    "    \"\"\"\n",
    "    return InfluxDBClient(url=url, token=token, org=org)\n",
    "\n",
    "def query_to_dataframe(client, query, field = \"location\"):\n",
    "    \"\"\"\n",
    "    구성된 쿼리를 실행하고 전달받은 데이터를 Dataframe으로 만듭니다.\n",
    "    \n",
    "    :param field: 기본값 location\n",
    "    :param client: InfluxDBClient\n",
    "    :param query: 요청할 쿼리\n",
    "    :return: DataFrame \n",
    "    \"\"\"\n",
    "    result = client.query_api().query(query=query)\n",
    "    results = []\n",
    "    \n",
    "    for table in result:\n",
    "        for record in table.records:\n",
    "            results.append({\n",
    "                \"time\": record.get_time(),\n",
    "                \"value\": round(record.get_value(), 2), # 반올림 추가\n",
    "                \"place\": record.values.get(\"place\"),\n",
    "                \"location\": record.values.get(field),\n",
    "                \"device\": record.values.get(\"device\")\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df['time'] = df['time'].astype(str).str.replace(r'\\+00:00$', '', regex=True)\n",
    "    return df\n",
    "\n",
    "def save_csv(df, file_pattern, directory):\n",
    "    \n",
    "    \"\"\"\n",
    "    DataFrame을 CSV로 변환하여 저장합니다.\n",
    "    \n",
    "    :param df: DataFrame\n",
    "    :param file_pattern: 파일 이름 패턴\n",
    "    :param directory: 저장할 위치\n",
    "    \"\"\"\n",
    "    df = df.round(2) # 반올림 추가\n",
    "    # 파일 경로를 확인 하고 없다면 생성 합니다.\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    previous_date = datetime.now() - timedelta(days=1)\n",
    "    filename = f\"{directory}{previous_date.strftime(file_pattern)}\"\n",
    "    df.to_csv(filename, index=False)\n",
    "        \n",
    "def update_location(df):\n",
    "    \"\"\"\n",
    "    환경 센서 Data에서 Device ID를 확인 하여 'location' 열을 업데이트 합니다.\n",
    "    \n",
    "    :param df: 환경 센서 DataFrame\n",
    "    :return: 'location' 열을 업데이트한 DataFrame\n",
    "    \"\"\"\n",
    "    df['location'] = df['device'].map(location_mapping)\n",
    "    return df\n",
    "\n",
    "def merge_data(directory_path):\n",
    "    \"\"\"\n",
    "    CSV 파일들을 하나의 DataFrame으로 병합하고 정렬 합니다.\n",
    "    \n",
    "    :param directory_path: 파일 경로 \n",
    "    :return: 병합된 DataFrame\n",
    "    \"\"\"\n",
    "    # 파일 내의 모든 CSV 파일 목록을 생성\n",
    "    csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "    # 모든 CSV 파일을 DataFrame으로 읽어와 하나로 병합\n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.round(2) # 반올림 추가\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        data_frames.append(df)\n",
    "\n",
    "    merged_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # 해당 컬럼들로 재구성\n",
    "    columns_to_keep = ['time', 'value', 'place', 'location']\n",
    "    filtered_df = merged_df[columns_to_keep]\n",
    "\n",
    "    # 'place', 'time' 으로 정렬\n",
    "    sorted_df = filtered_df.sort_values(['place', 'time'])\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "def df_hourly_calculate(dataframe, change_column_name, calculate):\n",
    "    \"\"\"\n",
    "    DataFrame에서 value를 시간별로 calculate 함수에 따라 계산합니다.\n",
    "    계산 후 value 컬럼을 지정한 이름으로 변경합니다.\n",
    "        \n",
    "    :param dataframe: 계산할 DataFrame\n",
    "    :param change_column_name: 계산 후 value 컬럼을 지정한 이름으로 변경\n",
    "    :param calculate: 문자열 형태로 계산할 함수명 ('sum', 'mean', 'diff' 등)\n",
    "    :return: 시간별 계산된 DataFrame\n",
    "    \"\"\"\n",
    "    # 시간 컬럼이 datetime형태인지 확인하고 설정후 Index로 설정\n",
    "    if not pd.api.types.is_datetime64_any_dtype(dataframe.index):\n",
    "        dataframe['time'] = pd.to_datetime(dataframe['time'])\n",
    "        dataframe.set_index('time', inplace=True)\n",
    "\n",
    "    # 시간별로 그룹화하여 마지막 값을 선택 (누적값의 경우)\n",
    "    if calculate == 'diff':\n",
    "        # 누적값의 마지막 값을 시간별로 선택하고 차분 계산\n",
    "        result = dataframe['value'].resample('H').last().diff().rename(change_column_name)\n",
    "    else:\n",
    "        # 기타 계산\n",
    "        result = dataframe.resample('H').agg({'value': calculate}).rename(columns={'value': change_column_name})\n",
    "\n",
    "    # Null 값 채우기 (forward fill)\n",
    "    result.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "def merge_dataframes(df_list):\n",
    "    \"\"\"\n",
    "    여러 DataFrame을 병합합니다. \n",
    "    \n",
    "    :param df_list: 병합할 DataFrame 리스트\n",
    "    :return: 병합된 DataFrame\n",
    "    \"\"\"\n",
    "    merged_df = df_list[0]\n",
    "    for df in df_list[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, left_index=True, right_index=True, how='outer', validate=None)\n",
    "    return merged_df\n",
    "\n",
    "def calculate_occupants(dataframe):\n",
    "    # time 컬럼을 datetime 타입으로 변환\n",
    "    dataframe['time'] = pd.to_datetime(dataframe['time'])\n",
    "\n",
    "    # 시간대별로 그룹화 (시간, 분, 초까지 모두 고려)\n",
    "    grouped = dataframe.groupby(['time', 'location'])['value'].sum().unstack().fillna(0)\n",
    "\n",
    "    # 입장과 퇴장 데이터 분리\n",
    "    grouped['in'] = grouped.filter(like='_in').sum(axis=1)\n",
    "    grouped['out'] = grouped.filter(like='_out').sum(axis=1)\n",
    "\n",
    "    # 순 입장 인원 계산 (재실 인원)\n",
    "    grouped['net'] = grouped['in'] - grouped['out']\n",
    "\n",
    "    # 누적 재실 인원 계산\n",
    "    cumulative = 0  # 초기값 설정\n",
    "    cumulative_results = []  # 누적 결과를 저장할 리스트\n",
    "    for net in grouped['net']:\n",
    "        cumulative += net\n",
    "        if cumulative < 0:\n",
    "            cumulative = 0  # 음수는 0으로 처리\n",
    "        cumulative_results.append(cumulative)\n",
    "\n",
    "    grouped['class_A_occupants'] = cumulative_results\n",
    "\n",
    "    return grouped[['class_A_occupants']]\n",
    "\n",
    "def separation_dataframe(dataframe):\n",
    "    \n",
    "    # 'location' 별로 데이터 프레임 그룹화\n",
    "    grouped = dataframe.groupby('location')\n",
    "\n",
    "    # 딕셔너리에 데이터 프레임 저장\n",
    "    dataframes = {location.replace(\" \", \"_\").lower(): df for location, df in grouped}\n",
    "\n",
    "    return dataframes"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 쿼리를 통한 데이터 수집\n",
    "InfluxDB에 연결하여 해당 쿼리들을 통해 아래 내용의 데이터를 수집한다.\n",
    "\n",
    "전력(W), 전력 사용량(Kwh), 온도, CO2, 조도, 습도, 이동 감지 카운터"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:53.656976Z",
     "start_time": "2024-05-05T06:31:50.926859Z"
    }
   },
   "source": [
    "# DB 연결 및 쿼리 실행\n",
    "client = create_client(url, token, org)\n",
    "\n",
    "# 전력(W) 조회(class a : main) Flux 쿼리\n",
    "query_power_total_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"powermetrics_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"description\"] == \"w\")\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"phase\"] == \"total\")\n",
    "  |> filter(fn: (r) => r[\"location\"] == \"main\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 전력 사용량 조회(class a : main) Flux 쿼리\n",
    "query_power_usage_total_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"powermetrics_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"phase\"] == \"kwh\")\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"description\"] == \"sum\")\n",
    "  |> filter(fn: (r) => r[\"location\"] == \"main\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 전력(W) 조회(class a : device) Flux 쿼리\n",
    "query_power_device_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"powermetrics_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"phase\"] == \"total\")\n",
    "  |> filter(fn: (r) => r[\"description\"] == \"w\")\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"location\"] != \"main\" and r[\"location\"] != \"outdoor_unit_room_light\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 전력 사용량 조회(class a : device) Flux 쿼리\n",
    "query_power_usage_device_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"powermetrics_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"phase\"] == \"kwh\")\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"description\"] == \"sum\")\n",
    "  |> filter(fn: (r) => r[\"location\"] != \"main\" and r[\"location\"] != \"outdoor_unit_room_light\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 온도 조회 Flux 쿼리\n",
    "query_temperature_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"environmentalsensors_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"measurement\"] == \"temperature\")\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 이산화탄소 조회 Flux 쿼리\n",
    "query_co2_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"environmentalsensors_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"measurement\"] == \"co2\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 조도 조회 Flux 쿼리\n",
    "query_illumination_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"environmentalsensors_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"measurement\"] == \"illumination\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 습도 조회 Flux 쿼리\n",
    "query_humidity_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"environmentalsensors_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"measurement\"] == \"humidity\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 이동 감지 카운터 조회 Flux 쿼리\n",
    "query_counter_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"milesight\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"_field\"] == \"line_periodic_data_1_out\" or r[\"_field\"] == \"line_periodic_data_1_in\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"_field\", \"device\"])\n",
    "'''\n",
    "\n",
    "print(query_counter_data)\n",
    "\n",
    "# 전력 CSV 생성(main)\n",
    "df_power = query_to_dataframe(client, query_power_total_data)\n",
    "print(df_power.head(2))\n",
    "save_csv(df_power, \"%m_%d_power_total_data.csv\", \"all_data/power/total/\")\n",
    "\n",
    "# 전력 사용량 CSV 생성(main)\n",
    "df_power_usage = query_to_dataframe(client, query_power_usage_total_data)\n",
    "save_csv(df_power_usage, \"%m_%d_power_usage_total_data.csv\", \"all_data/power_usage/total/\")\n",
    "print(df_power_usage.head(2))\n",
    "\n",
    "# 전력 CSV 생성(device)\n",
    "df_power_device = query_to_dataframe(client, query_power_device_data)\n",
    "print(df_power_device.head(2))\n",
    "save_csv(df_power_device, \"%m_%d_power_device_data.csv\", \"all_data/power/device/\")\n",
    "\n",
    "# 전력 사용량 CSV 생성(device)\n",
    "df_power_usage_device = query_to_dataframe(client, query_power_usage_device_data)\n",
    "save_csv(df_power_usage_device, \"%m_%d_power_usage_device_data.csv\", \"all_data/power_usage/device/\")\n",
    "print(df_power_usage_device.head(2))\n",
    "\n",
    "# 온도 CSV 생성\n",
    "df_temperature = query_to_dataframe(client, query_temperature_data)\n",
    "df_temperature_fix = update_location(df_temperature)\n",
    "print(df_temperature_fix.head(2))\n",
    "save_csv(df_temperature_fix, \"%m_%d_temperature_data.csv\", \"all_data/temperature/\")\n",
    "\n",
    "# co2 CSV 생성\n",
    "df_co2 = query_to_dataframe(client, query_co2_data)\n",
    "df_co2_fix = update_location(df_co2)\n",
    "print(df_co2_fix.head(2))\n",
    "save_csv(df_co2_fix, \"%m_%d_co2_data.csv\", \"all_data/co2/\")\n",
    "\n",
    "# 조도 CSV 생성\n",
    "df_illumination = query_to_dataframe(client, query_illumination_data)\n",
    "update_location(df_illumination)\n",
    "print(df_illumination.head(2))\n",
    "save_csv(df_illumination, \"%m_%d_illumination_data.csv\", \"all_data/illumination/\")\n",
    "\n",
    "# 습도 CSV 생성\n",
    "df_humidity = query_to_dataframe(client, query_humidity_data)\n",
    "update_location(df_humidity)\n",
    "print(df_humidity.head(2))\n",
    "save_csv(df_humidity, \"%m_%d_humidity_data.csv\", \"all_data/humidity/\")\n",
    "\n",
    "# 이동 감지 카운터 CSV 생성\n",
    "df_counter = query_to_dataframe(client, query_counter_data, \"_field\")\n",
    "print(df_counter.head(2))\n",
    "save_csv(df_counter, \"%m_%d_counter_data.csv\", \"all_data/counter/total/\")\n",
    "\n",
    "# 연결 Client 종료\n",
    "client.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import \"experimental\"\n",
      "from(bucket: \"milesight\")\n",
      "  |> range(start: 2024-05-03T15:00:00Z, stop: 2024-05-04T15:00:00Z)\n",
      "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
      "  |> filter(fn: (r) => r[\"_field\"] == \"line_periodic_data_1_out\" or r[\"_field\"] == \"line_periodic_data_1_in\")\n",
      "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
      "  |> map(fn: (r) => ({r with _time: experimental.addDuration(d: 9h, to: r._time)}))\n",
      "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"_field\", \"device\"])\n",
      "\n",
      "                  time   value    place location     device\n",
      "0  2024-05-04 00:02:00  1148.0  class_a     main  gems-3500\n",
      "1  2024-05-04 00:04:00  1090.5  class_a     main  gems-3500\n",
      "                  time  value    place location     device\n",
      "0  2024-05-04 00:02:00  836.6  class_a     main  gems-3500\n",
      "1  2024-05-04 00:04:00  836.6  class_a     main  gems-3500\n",
      "                  time  value    place        location     device\n",
      "0  2024-05-04 00:02:00   58.0  class_a  ac_indoor_unit  gems-3500\n",
      "1  2024-05-04 00:04:00   60.0  class_a  ac_indoor_unit  gems-3500\n",
      "                  time  value    place        location     device\n",
      "0  2024-05-04 00:02:00   43.5  class_a  ac_indoor_unit  gems-3500\n",
      "1  2024-05-04 00:04:00   43.5  class_a  ac_indoor_unit  gems-3500\n",
      "                  time  value    place             location            device\n",
      "0  2024-05-04 00:02:00   23.2  class_a  bottom_right_corner  24e124126d152969\n",
      "1  2024-05-04 00:04:00   23.1  class_a  bottom_right_corner  24e124126d152969\n",
      "                  time  value    place location            device\n",
      "0  2024-05-04 00:02:00  623.5  class_a   indoor  24e124128c067999\n",
      "1  2024-05-04 00:04:00  626.0  class_a   indoor  24e124128c067999\n",
      "                  time  value    place location            device\n",
      "0  2024-05-04 00:02:00   67.0  class_a   indoor  24e124128c067999\n",
      "1  2024-05-04 00:04:00   66.0  class_a   indoor  24e124128c067999\n",
      "                  time  value    place             location            device\n",
      "0  2024-05-04 00:02:00   43.5  class_a  bottom_right_corner  24e124126d152969\n",
      "1  2024-05-04 00:04:00   43.5  class_a  bottom_right_corner  24e124126d152969\n",
      "                  time  value    place                 location device\n",
      "0  2024-05-04 01:00:00    0.0  class_a  line_periodic_data_1_in  vs133\n",
      "1  2024-05-04 02:00:00    0.0  class_a  line_periodic_data_1_in  vs133\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CSV 병합 \n",
    "\n",
    "파일 경로를 통해 집계된 CSV를 병합하고 각각의 Dataframe으로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:53.846778Z",
     "start_time": "2024-05-05T06:31:53.659545Z"
    }
   },
   "source": [
    "power_path = 'all_data/power/total'\n",
    "power_total_df = merge_data(power_path)\n",
    "print(power_total_df.head(2), \"\\n\")\n",
    "\n",
    "power_usage_path = 'all_data/power_usage/total'\n",
    "power_usage_total_df = merge_data(power_usage_path)\n",
    "print(power_usage_total_df.head(2), \"\\n\")\n",
    "\n",
    "power_device_path = 'all_data/power/device'\n",
    "power_device_df = merge_data(power_device_path)\n",
    "print(power_device_df.head(2), \"\\n\")\n",
    "power_dataframes = separation_dataframe(power_device_df)\n",
    "\n",
    "power_usage_device_path = 'all_data/power_usage/device'\n",
    "power_usage_device_df = merge_data(power_usage_device_path)\n",
    "print(power_usage_device_df.head(2), \"\\n\")\n",
    "power_usage_dataframes = separation_dataframe(power_usage_device_df)\n",
    "\n",
    "temperature_path = 'all_data/temperature'\n",
    "temperature_total_df = merge_data(temperature_path)\n",
    "print(temperature_total_df.head(2), \"\\n\")\n",
    "\n",
    "co2_path = 'all_data/co2'\n",
    "co2_total_df = merge_data(co2_path)\n",
    "print(co2_total_df.head(2), \"\\n\")\n",
    "\n",
    "illumination_path = 'all_data/illumination'\n",
    "illumination_total_df = merge_data(illumination_path)\n",
    "print(illumination_total_df.head(2), \"\\n\")\n",
    "\n",
    "humidity_path = 'all_data/humidity'\n",
    "humidity_total_df = merge_data(humidity_path)\n",
    "print(humidity_total_df.head(2), \"\\n\")\n",
    "\n",
    "counter_path = 'all_data/counter/total'\n",
    "counter_total_df = merge_data(counter_path)\n",
    "print(counter_total_df.head(2), \"\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time   value    place location\n",
      "0 2024-05-04 00:02:00  1148.0  class_a     main\n",
      "1 2024-05-04 00:04:00  1090.5  class_a     main \n",
      "\n",
      "                 time  value    place location\n",
      "0 2024-05-04 00:02:00  836.6  class_a     main\n",
      "1 2024-05-04 00:04:00  836.6  class_a     main \n",
      "\n",
      "                   time   value    place         location\n",
      "0   2024-05-04 00:02:00    58.0  class_a   ac_indoor_unit\n",
      "720 2024-05-04 00:02:00  1077.5  class_a  ac_outdoor_unit \n",
      "\n",
      "                   time  value    place         location\n",
      "0   2024-05-04 00:02:00   43.5  class_a   ac_indoor_unit\n",
      "720 2024-05-04 00:02:00  773.2  class_a  ac_outdoor_unit \n",
      "\n",
      "                   time  value    place             location\n",
      "0   2024-05-04 00:02:00   23.2  class_a  bottom_right_corner\n",
      "704 2024-05-04 00:02:00   24.0  class_a               indoor \n",
      "\n",
      "                 time  value    place location\n",
      "0 2024-05-04 00:02:00  623.5  class_a   indoor\n",
      "1 2024-05-04 00:04:00  626.0  class_a   indoor \n",
      "\n",
      "                 time  value    place location\n",
      "0 2024-05-04 00:02:00   67.0  class_a   indoor\n",
      "1 2024-05-04 00:04:00   66.0  class_a   indoor \n",
      "\n",
      "                   time  value    place             location\n",
      "0   2024-05-04 00:02:00   43.5  class_a  bottom_right_corner\n",
      "704 2024-05-04 00:02:00   38.5  class_a               indoor \n",
      "\n",
      "                  time  value    place                  location\n",
      "0  2024-05-04 01:00:00    0.0  class_a   line_periodic_data_1_in\n",
      "24 2024-05-04 01:00:00    0.0  class_a  line_periodic_data_1_out \n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataframe 전처리\n",
    "Dataframe을 1시간 단위로 목적에 맞는 형태로 연산한다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:53.927260Z",
     "start_time": "2024-05-05T06:31:53.850224Z"
    }
   },
   "source": [
    "# 평균 온도\n",
    "temperature_total_df = df_hourly_calculate(temperature_total_df, \"average_temperature(°C)\", \"mean\")\n",
    "\n",
    "# 평균 습도\n",
    "humidity_total_df = df_hourly_calculate(humidity_total_df, \"average_humidity(%)\", \"mean\")\n",
    "\n",
    "# 평균 co2\n",
    "co2_total_df = df_hourly_calculate(co2_total_df, \"average_co2(ppm)\", \"mean\")\n",
    "\n",
    "# 평균 조도\n",
    "illumination_total_df = df_hourly_calculate(illumination_total_df, \"average_illumination(lux)\", \"mean\")\n",
    "\n",
    "# class a 재실 인원 추론\n",
    "counter_total_df = calculate_occupants(counter_total_df)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n",
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n",
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n",
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:54.021330Z",
     "start_time": "2024-05-05T06:31:53.930645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 전체 전력/전력량\n",
    "power_total_df = df_hourly_calculate(power_total_df, \"total_power(Wh)\", \"sum\")\n",
    "power_usage_total_df = df_hourly_calculate(power_usage_total_df, 'total_power_usage(Kwh)', 'diff')\n",
    "\n",
    "# 실내기 전력/전력량\n",
    "power_ac_in_df = df_hourly_calculate(power_dataframes['ac_indoor_unit'], \"ac_in_power(Wh)\", \"sum\")\n",
    "power_usage_ac_in_df = df_hourly_calculate(power_usage_dataframes['ac_indoor_unit'], \"ac_in_power(Kwh)\", \"diff\")\n",
    "\n",
    "# 실외기 전력/전력량\n",
    "power_ac_out_df = df_hourly_calculate(power_dataframes['ac_outdoor_unit'], \"ac_out_power(Wh)\", \"sum\")\n",
    "power_usage_ac_out_df = df_hourly_calculate(power_usage_dataframes['ac_outdoor_unit'], \"ac_out_power(Kwh)\", \"diff\")\n",
    "\n",
    "# 자동문 전력/전력량\n",
    "power_automatic_door_df = df_hourly_calculate(power_dataframes['automatic_door'], \"automatic_door_power(Wh)\", \"sum\")\n",
    "power_usage_automatic_door_df = df_hourly_calculate(power_usage_dataframes['automatic_door'], \"automatic_door_power(Kwh)\", \"diff\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n",
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n",
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n",
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n",
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n",
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n",
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n",
      "/var/folders/y3/2dstyvvd50d1nxh5n95ytzzw0000gn/T/ipykernel_4286/1507563954.py:119: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result.fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataframe 병합\n",
    "\n",
    "데이터 분석을 위해 공통 분모를 가진 데이터끼리 병합하여 하나의 CSV로 만든다. "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:54.305506Z",
     "start_time": "2024-05-05T06:31:54.033043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataframe 병합 ( 환경 센서 )\n",
    "final_environment_data_df = merge_dataframes([counter_total_df, temperature_total_df, humidity_total_df, co2_total_df, illumination_total_df])\n",
    "\n",
    "# 결과 확인\n",
    "print(final_environment_data_df)\n",
    "\n",
    "# 인덱스를 일반 컬럼으로 변경\n",
    "final_environment_data_df = final_environment_data_df.reset_index()\n",
    "\n",
    "# CSV 파일로 저장\n",
    "final_environment_data_df.to_csv('all_data/all_final_df/class_a_environmental_sensor.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     class_A_occupants  average_temperature(°C)  \\\n",
      "time                                                              \n",
      "2024-05-04 00:00:00                NaN                23.484186   \n",
      "2024-05-04 01:00:00                0.0                23.580889   \n",
      "2024-05-04 02:00:00                0.0                23.596163   \n",
      "2024-05-04 03:00:00                0.0                23.568750   \n",
      "2024-05-04 04:00:00                0.0                23.521124   \n",
      "2024-05-04 05:00:00                0.0                23.471111   \n",
      "2024-05-04 06:00:00                0.0                23.430682   \n",
      "2024-05-04 07:00:00                0.0                23.383708   \n",
      "2024-05-04 08:00:00                0.0                23.370455   \n",
      "2024-05-04 09:00:00                0.0                23.530000   \n",
      "2024-05-04 10:00:00                0.0                23.831111   \n",
      "2024-05-04 11:00:00                0.0                24.051685   \n",
      "2024-05-04 12:00:00                0.0                24.155222   \n",
      "2024-05-04 13:00:00                0.0                24.360778   \n",
      "2024-05-04 14:00:00                1.0                24.669101   \n",
      "2024-05-04 15:00:00                1.0                25.012222   \n",
      "2024-05-04 16:00:00                5.0                24.776742   \n",
      "2024-05-04 17:00:00                4.0                24.657727   \n",
      "2024-05-04 18:00:00                5.0                24.377273   \n",
      "2024-05-04 19:00:00                5.0                24.103778   \n",
      "2024-05-04 20:00:00                1.0                23.865730   \n",
      "2024-05-04 21:00:00                0.0                23.601111   \n",
      "2024-05-04 22:00:00                4.0                23.436333   \n",
      "2024-05-04 23:00:00                4.0                23.453523   \n",
      "2024-05-05 00:00:00                4.0                23.383333   \n",
      "\n",
      "                     average_humidity(%)  average_co2(ppm)  \\\n",
      "time                                                         \n",
      "2024-05-04 00:00:00            40.709302        590.097586   \n",
      "2024-05-04 01:00:00            40.293556        554.133333   \n",
      "2024-05-04 02:00:00            39.947674        534.566667   \n",
      "2024-05-04 03:00:00            40.292614        530.000000   \n",
      "2024-05-04 04:00:00            40.278989        532.405667   \n",
      "2024-05-04 05:00:00            40.094444        526.189000   \n",
      "2024-05-04 06:00:00            40.246250        529.089000   \n",
      "2024-05-04 07:00:00            40.189101        528.283333   \n",
      "2024-05-04 08:00:00            39.963068        521.883333   \n",
      "2024-05-04 09:00:00            39.969101        520.500000   \n",
      "2024-05-04 10:00:00            39.580556        515.705667   \n",
      "2024-05-04 11:00:00            40.586966        505.716667   \n",
      "2024-05-04 12:00:00            43.265667        503.038667   \n",
      "2024-05-04 13:00:00            44.278667        517.477667   \n",
      "2024-05-04 14:00:00            44.174157        528.266667   \n",
      "2024-05-04 15:00:00            44.328667        533.078000   \n",
      "2024-05-04 16:00:00            44.420449        594.833333   \n",
      "2024-05-04 17:00:00            44.483864        602.528000   \n",
      "2024-05-04 18:00:00            44.241477        617.783333   \n",
      "2024-05-04 19:00:00            45.016667        635.316333   \n",
      "2024-05-04 20:00:00            45.921348        599.422333   \n",
      "2024-05-04 21:00:00            45.330556        552.083333   \n",
      "2024-05-04 22:00:00            45.008333        595.389000   \n",
      "2024-05-04 23:00:00            45.542614        672.789000   \n",
      "2024-05-05 00:00:00            45.833333        679.000000   \n",
      "\n",
      "                     average_illumination(lux)  \n",
      "time                                            \n",
      "2024-05-04 00:00:00                  32.741379  \n",
      "2024-05-04 01:00:00                   0.000000  \n",
      "2024-05-04 02:00:00                   0.000000  \n",
      "2024-05-04 03:00:00                   0.000000  \n",
      "2024-05-04 04:00:00                   0.000000  \n",
      "2024-05-04 05:00:00                   0.300000  \n",
      "2024-05-04 06:00:00                   1.722333  \n",
      "2024-05-04 07:00:00                   1.000000  \n",
      "2024-05-04 08:00:00                   1.000000  \n",
      "2024-05-04 09:00:00                   1.000000  \n",
      "2024-05-04 10:00:00                   1.000000  \n",
      "2024-05-04 11:00:00                   1.000000  \n",
      "2024-05-04 12:00:00                   1.000000  \n",
      "2024-05-04 13:00:00                  65.766667  \n",
      "2024-05-04 14:00:00                  68.900000  \n",
      "2024-05-04 15:00:00                  68.866667  \n",
      "2024-05-04 16:00:00                  69.266667  \n",
      "2024-05-04 17:00:00                  66.666667  \n",
      "2024-05-04 18:00:00                  65.766667  \n",
      "2024-05-04 19:00:00                  67.111000  \n",
      "2024-05-04 20:00:00                  66.511000  \n",
      "2024-05-04 21:00:00                  65.200000  \n",
      "2024-05-04 22:00:00                  65.400000  \n",
      "2024-05-04 23:00:00                  66.200000  \n",
      "2024-05-05 00:00:00                  67.000000  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'all_data/all_final_df'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m final_environment_data_df \u001B[38;5;241m=\u001B[39m final_environment_data_df\u001B[38;5;241m.\u001B[39mreset_index()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# CSV 파일로 저장\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m final_environment_data_df\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall_data/all_final_df/class_a_environmental_sensor.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:3902\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[1;32m   3891\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[1;32m   3893\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[1;32m   3894\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[1;32m   3895\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3899\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[1;32m   3900\u001B[0m )\n\u001B[0;32m-> 3902\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrameRenderer(formatter)\u001B[38;5;241m.\u001B[39mto_csv(\n\u001B[1;32m   3903\u001B[0m     path_or_buf,\n\u001B[1;32m   3904\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[1;32m   3905\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[1;32m   3906\u001B[0m     encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[1;32m   3907\u001B[0m     errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[1;32m   3908\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   3909\u001B[0m     quoting\u001B[38;5;241m=\u001B[39mquoting,\n\u001B[1;32m   3910\u001B[0m     columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[1;32m   3911\u001B[0m     index_label\u001B[38;5;241m=\u001B[39mindex_label,\n\u001B[1;32m   3912\u001B[0m     mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[1;32m   3913\u001B[0m     chunksize\u001B[38;5;241m=\u001B[39mchunksize,\n\u001B[1;32m   3914\u001B[0m     quotechar\u001B[38;5;241m=\u001B[39mquotechar,\n\u001B[1;32m   3915\u001B[0m     date_format\u001B[38;5;241m=\u001B[39mdate_format,\n\u001B[1;32m   3916\u001B[0m     doublequote\u001B[38;5;241m=\u001B[39mdoublequote,\n\u001B[1;32m   3917\u001B[0m     escapechar\u001B[38;5;241m=\u001B[39mescapechar,\n\u001B[1;32m   3918\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m   3919\u001B[0m )\n",
      "File \u001B[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/pandas/io/formats/format.py:1152\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[1;32m   1131\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1133\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[1;32m   1134\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[1;32m   1135\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1150\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[1;32m   1151\u001B[0m )\n\u001B[0;32m-> 1152\u001B[0m csv_formatter\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1154\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[1;32m   1155\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "File \u001B[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/pandas/io/formats/csvs.py:247\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;124;03mCreate the writer & save.\u001B[39;00m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[0;32m--> 247\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_handle(\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilepath_or_buffer,\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m    250\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[1;32m    251\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors,\n\u001B[1;32m    252\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression,\n\u001B[1;32m    253\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstorage_options,\n\u001B[1;32m    254\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[1;32m    256\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[1;32m    257\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[1;32m    258\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    263\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[1;32m    264\u001B[0m     )\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
      "File \u001B[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:739\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    737\u001B[0m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[0;32m--> 739\u001B[0m     check_parent_directory(\u001B[38;5;28mstr\u001B[39m(handle))\n\u001B[1;32m    741\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[1;32m    742\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzstd\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    743\u001B[0m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:604\u001B[0m, in \u001B[0;36mcheck_parent_directory\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    602\u001B[0m parent \u001B[38;5;241m=\u001B[39m Path(path)\u001B[38;5;241m.\u001B[39mparent\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent\u001B[38;5;241m.\u001B[39mis_dir():\n\u001B[0;32m--> 604\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot save file into a non-existent directory: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mOSError\u001B[0m: Cannot save file into a non-existent directory: 'all_data/all_final_df'"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:31:54.311871Z",
     "start_time": "2024-05-05T06:31:54.309489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataframe 병합 ( 전기 센서 )\n",
    "final_main_power_data_df = merge_dataframes([power_total_df])\n",
    "print(final_main_power_data_df)\n",
    "final_main_power_data_df = final_main_power_data_df.reset_index()\n",
    "final_main_power_data_df.to_csv('all_data/all_final_df/class_a_main_power_sensor.csv', index=False)\n",
    "\n",
    "final_main_power_usage_data_df = merge_dataframes([power_usage_total_df])\n",
    "print(final_main_power_usage_data_df)\n",
    "final_main_power_usage_data_df = final_main_power_usage_data_df.reset_index()\n",
    "final_main_power_usage_data_df.to_csv('all_data/all_final_df/class_a_main_power_usage_sensor.csv', index=False)\n",
    "\n",
    "final_device_power_data_df = merge_dataframes([power_ac_in_df, power_ac_out_df, power_automatic_door_df])\n",
    "print(final_device_power_data_df)\n",
    "final_power_data_df = final_device_power_data_df.reset_index()\n",
    "final_power_data_df.to_csv('all_data/all_final_df/class_a_device_power_sensor.csv', index=False)\n",
    "\n",
    "final_device_power_usage_df = merge_dataframes([power_usage_ac_in_df, power_usage_ac_out_df, power_usage_automatic_door_df])\n",
    "print(final_device_power_usage_df)\n",
    "final_power_usage_df = final_device_power_usage_df.reset_index()\n",
    "final_power_usage_df.to_csv('all_data/all_final_df/class_a_device_power_usage_sensor.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoothing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
