{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## InfluxDB 연결 설정\n",
    "InfluxDB 연결 정보 설정 및 장비 맵핑"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T01:05:08.600049Z",
     "start_time": "2024-05-02T01:05:06.340624Z"
    }
   },
   "source": [
    "%pip install influxdb_client"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: influxdb_client in /home/nhnacademy/anaconda3/lib/python3.9/site-packages (1.42.0)\r\n",
      "Requirement already satisfied: reactivex>=4.0.4 in /home/nhnacademy/anaconda3/lib/python3.9/site-packages (from influxdb_client) (4.0.4)\r\n",
      "Requirement already satisfied: certifi>=14.05.14 in /home/nhnacademy/anaconda3/lib/python3.9/site-packages (from influxdb_client) (2024.2.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/nhnacademy/anaconda3/lib/python3.9/site-packages (from influxdb_client) (2.8.2)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /home/nhnacademy/anaconda3/lib/python3.9/site-packages (from influxdb_client) (68.0.0)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/nhnacademy/anaconda3/lib/python3.9/site-packages (from influxdb_client) (1.26.16)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/nhnacademy/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.5.3->influxdb_client) (1.16.0)\r\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /home/nhnacademy/anaconda3/lib/python3.9/site-packages (from reactivex>=4.0.4->influxdb_client) (4.7.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T01:05:08.790989Z",
     "start_time": "2024-05-02T01:05:08.604297Z"
    }
   },
   "source": [
    "import os\n",
    "import pytz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime, timedelta\n",
    "from influxdb_client import InfluxDBClient"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T02:20:57.897658Z",
     "start_time": "2024-05-02T02:20:57.639945Z"
    }
   },
   "source": [
    "# InfluxDB 설정 정보\n",
    "url = \"http://133.186.144.22:8086\"\n",
    "token = \"r3Ecro-rJQ82UpyNScnHXYDZ3KaE45AzweCXz6QIv2jeo7eOP4hL4-A9uKvkAVQDg_xavWorGUGZn7MI_sPCwg==\"\n",
    "org = \"smoothing\"\n",
    "\n",
    "# Device ID와 위치를 매핑\n",
    "location_mapping = {\n",
    "    '24e124126d152919': 'indoor',\n",
    "    '24e124126d152969': 'bottom_right_corner',\n",
    "    '24e124128c067999': 'indoor',\n",
    "    '24e124785c389818': 'bottom_left_corner',\n",
    "    '24e124785c421885': 'top_right_corner'\n",
    "}\n",
    "\n",
    "# 한국 시간대 설정\n",
    "korea_tz = pytz.timezone('Asia/Seoul')\n",
    "\n",
    "# 실행 시점의 날짜를 기준으로 전날의 날짜를 계산\n",
    "today_kst = datetime.now(korea_tz)\n",
    "yesterday_kst = today_kst - timedelta(days=1)\n",
    "\n",
    "# 전날의 시작과 끝 시간을 한국 시간대로 설정\n",
    "start_time_kst = korea_tz.localize(datetime(yesterday_kst.year, yesterday_kst.month, yesterday_kst.day, 0, 0, 0))\n",
    "end_time_kst = start_time_kst + timedelta(days=1)\n",
    "\n",
    "# UTC로 시간 변환\n",
    "start_time_utc = start_time_kst.astimezone(pytz.utc)\n",
    "end_time_utc = end_time_kst.astimezone(pytz.utc)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T02:21:03.346562Z",
     "start_time": "2024-05-02T02:21:03.333409Z"
    }
   },
   "source": [
    "def create_client(url, token, org):\n",
    "    \"\"\"\n",
    "    Influx DB 연결 Client를 생성합니다.\n",
    "    \n",
    "    :param url: InfluxDB 연결 주소\n",
    "    :param token: InfluxDB 토큰\n",
    "    :param org:  InfluxDB 조직\n",
    "    :return: InfluxDBClient\n",
    "    \"\"\"\n",
    "    return InfluxDBClient(url=url, token=token, org=org)\n",
    "\n",
    "def query_to_dataframe(client, query, field = \"location\"):\n",
    "    \"\"\"\n",
    "    구성된 쿼리를 실행하고 전달받은 데이터를 Dataframe으로 만듭니다.\n",
    "    \n",
    "    :param field: 기본값 location\n",
    "    :param client: InfluxDBClient\n",
    "    :param query: 요청할 쿼리\n",
    "    :return: DataFrame \n",
    "    \"\"\"\n",
    "    result = client.query_api().query(query=query)\n",
    "    results = []\n",
    "    \n",
    "    for table in result:\n",
    "        for record in table.records:\n",
    "            results.append({\n",
    "                \"time\": record.get_time(),\n",
    "                \"value\": record.get_value(),\n",
    "                \"place\": record.values.get(\"place\"),\n",
    "                \"location\": record.values.get(field),\n",
    "                \"device\": record.values.get(\"device\")\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df['time'] = df['time'].astype(str).str.replace(r'\\+00:00$', '', regex=True)\n",
    "    return df\n",
    "\n",
    "def save_csv(df, file_pattern, directory):\n",
    "    \n",
    "    \"\"\"\n",
    "    DataFrame을 CSV로 변환하여 저장합니다.\n",
    "    \n",
    "    :param df: DataFrame\n",
    "    :param file_pattern: 파일 이름 패턴\n",
    "    :param directory: 저장할 위치\n",
    "    \"\"\"\n",
    "    # 파일 경로를 확인 하고 없다면 생성 합니다.\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    previous_date = datetime.now() - timedelta(days=1)\n",
    "    filename = f\"{directory}{previous_date.strftime(file_pattern)}\"\n",
    "    df.to_csv(filename, index=False)\n",
    "        \n",
    "def update_location(df):\n",
    "    \"\"\"\n",
    "    환경 센서 Data에서 Device ID를 확인 하여 'location' 열을 업데이트 합니다.\n",
    "    \n",
    "    :param df: 환경 센서 DataFrame\n",
    "    :return: 'location' 열을 업데이트한 DataFrame\n",
    "    \"\"\"\n",
    "    df['location'] = df['device'].map(location_mapping)\n",
    "    return df\n",
    "\n",
    "def merge_data(directory_path):\n",
    "    \"\"\"\n",
    "    CSV 파일들을 하나의 DataFrame으로 병합하고 정렬 합니다.\n",
    "    \n",
    "    :param directory_path: 파일 경로 \n",
    "    :return: 병합된 DataFrame\n",
    "    \"\"\"\n",
    "    # 파일 내의 모든 CSV 파일 목록을 생성\n",
    "    csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "    # 모든 CSV 파일을 DataFrame으로 읽어와 하나로 병합\n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        data_frames.append(df)\n",
    "\n",
    "    merged_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # 해당 컬럼들로 재구성\n",
    "    columns_to_keep = ['time', 'value', 'place', 'location']\n",
    "    filtered_df = merged_df[columns_to_keep]\n",
    "\n",
    "    # 'place', 'time' 으로 정렬\n",
    "    sorted_df = filtered_df.sort_values(['place', 'time'])\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "def df_hourly_calculate(dataframe, change_column_name, calculate):\n",
    "    \"\"\"\n",
    "    DataFrame에서 value를 시간별로 calculate 함수에 따라 계산합니다.\n",
    "    계산 후 value 컬럼을 지정한 이름으로 변경합니다.\n",
    "        \n",
    "    :param dataframe: 계산할 DataFrame\n",
    "    :param change_column_name: 계산 후 value 컬럼을 지정한 이름으로 변경\n",
    "    :param calculate: 문자열 형태로 계산할 함수명 ('sum', 'mean', 'diff' 등)\n",
    "    :return: 시간별 계산된 DataFrame\n",
    "    \"\"\"\n",
    "    # 시간 컬럼이 datetime형태인지 확인하고 설정후 Index로 설정\n",
    "    if not pd.api.types.is_datetime64_any_dtype(dataframe.index):\n",
    "        dataframe['time'] = pd.to_datetime(dataframe['time'])\n",
    "        dataframe.set_index('time', inplace=True)\n",
    "\n",
    "    # 시간별로 그룹화하여 마지막 값을 선택 (누적값의 경우)\n",
    "    if calculate == 'diff':\n",
    "        # 누적값의 마지막 값을 시간별로 선택하고 차분 계산\n",
    "        result = dataframe['value'].resample('H').last().diff().rename(change_column_name)\n",
    "    else:\n",
    "        # 기타 계산\n",
    "        result = dataframe.resample('H').agg({'value': calculate}).rename(columns={'value': change_column_name})\n",
    "\n",
    "    # Null 값 채우기 (forward fill)\n",
    "    result.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "def merge_dataframes(df_list):\n",
    "    \"\"\"\n",
    "    여러 DataFrame을 병합합니다. \n",
    "    \n",
    "    :param df_list: 병합할 DataFrame 리스트\n",
    "    :return: 병합된 DataFrame\n",
    "    \"\"\"\n",
    "    merged_df = df_list[0]\n",
    "    for df in df_list[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, left_index=True, right_index=True, how='outer', validate=None)\n",
    "    return merged_df\n",
    "\n",
    "def calculate_attendance(dataframe):\n",
    "    # time 컬럼을 datetime 타입으로 변환\n",
    "    dataframe['time'] = pd.to_datetime(dataframe['time'])\n",
    "\n",
    "    # 시간대별로 그룹화 (시간, 분, 초까지 모두 고려)\n",
    "    grouped = dataframe.groupby(['time', 'location'])['value'].sum().unstack().fillna(0)\n",
    "\n",
    "    # 입장과 퇴장 데이터 분리\n",
    "    grouped['in'] = grouped.filter(like='_in').sum(axis=1)\n",
    "    grouped['out'] = grouped.filter(like='_out').sum(axis=1)\n",
    "\n",
    "    # 순 입장 인원 계산 (재실 인원)\n",
    "    grouped['net'] = grouped['in'] - grouped['out']\n",
    "\n",
    "    # 누적 재실 인원 계산\n",
    "    cumulative = 0  # 초기값 설정\n",
    "    cumulative_results = []  # 누적 결과를 저장할 리스트\n",
    "    for net in grouped['net']:\n",
    "        cumulative += net\n",
    "        if cumulative < 0:\n",
    "            cumulative = 0  # 음수는 0으로 처리\n",
    "        cumulative_results.append(cumulative)\n",
    "\n",
    "    grouped['cumulative'] = cumulative_results\n",
    "\n",
    "    return grouped[['in', 'out', 'cumulative']]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T02:02:46.602292Z",
     "start_time": "2024-05-02T02:02:45.541172Z"
    }
   },
   "source": [
    "# DB 연결 및 쿼리 실행\n",
    "client = create_client(url, token, org)\n",
    "\n",
    "# 전력(W) 조회(class a : main) Flux 쿼리\n",
    "query_power_total_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"powermetrics_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"description\"] == \"w\")\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"phase\"] == \"total\")\n",
    "  |> filter(fn: (r) => r[\"location\"] == \"main\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 전력 사용량 조회(class a : main) Flux 쿼리\n",
    "query_power_usage_total_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"powermetrics_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"phase\"] == \"kwh\")\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"description\"] == \"sum\")\n",
    "  |> filter(fn: (r) => r[\"location\"] == \"main\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 온도 조회 Flux 쿼리\n",
    "query_temperature_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"environmentalsensors_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"measurement\"] == \"temperature\")\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 이산화탄소 조회 Flux 쿼리\n",
    "query_co2_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"environmentalsensors_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"measurement\"] == \"co2\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 조도 조회 Flux 쿼리\n",
    "query_illumination_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"environmentalsensors_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"measurement\"] == \"illumination\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 습도 조회 Flux 쿼리\n",
    "query_humidity_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"environmentalsensors_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"measurement\"] == \"humidity\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 이동 감지 카운터 조회 Flux 쿼리\n",
    "query_counter_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"milesight\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"_field\"] == \"line_periodic_data_1_out\" or r[\"_field\"] == \"line_periodic_data_1_in\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"_field\", \"device\"])\n",
    "'''\n",
    "\n",
    "print(query_counter_data)\n",
    "\n",
    "# 전력 CSV 생성\n",
    "df_power = query_to_dataframe(client, query_power_total_data)\n",
    "print(df_power.head(2))\n",
    "save_csv(df_power, \"%m_%d_power_total_data.csv\", \"all_data/power/total/\")\n",
    "\n",
    "# 전력 사용량 CSV 생성\n",
    "df_power_usage = query_to_dataframe(client, query_power_usage_total_data)\n",
    "save_csv(df_power_usage, \"%m_%d_power_usage_total_data.csv\", \"all_data/power_usage/total/\")\n",
    "print(df_power_usage.head(2))\n",
    "\n",
    "# 온도 CSV 생성\n",
    "df_temperature = query_to_dataframe(client, query_temperature_data)\n",
    "df_temperature_fix = update_location(df_temperature)\n",
    "print(df_temperature_fix.head(2))\n",
    "save_csv(df_temperature_fix, \"%m_%d_temperature_data.csv\", \"all_data/temperature/total/\")\n",
    "\n",
    "# co2 CSV 생성\n",
    "df_co2 = query_to_dataframe(client, query_co2_data)\n",
    "df_co2_fix = update_location(df_co2)\n",
    "print(df_co2_fix.head(2))\n",
    "save_csv(df_co2_fix, \"%m_%d_co2_data.csv\", \"all_data/co2/\")\n",
    "\n",
    "# 조도 CSV 생성\n",
    "df_illumination = query_to_dataframe(client, query_illumination_data)\n",
    "update_location(df_illumination)\n",
    "print(df_illumination.head(2))\n",
    "save_csv(df_illumination, \"%m_%d_illumination_data.csv\", \"all_data/illumination/\")\n",
    "\n",
    "# 습도 CSV 생성\n",
    "df_humidity = query_to_dataframe(client, query_humidity_data)\n",
    "update_location(df_humidity)\n",
    "print(df_humidity.head(2))\n",
    "save_csv(df_humidity, \"%m_%d_humidity_data.csv\", \"all_data/humidity/\")\n",
    "\n",
    "# 이동 감지 카운터 CSV 생성\n",
    "df_counter = query_to_dataframe(client, query_counter_data, \"_field\")\n",
    "print(df_counter.head(2))\n",
    "save_csv(df_counter, \"%m_%d_counter_data.csv\", \"all_data/counter/total/\")\n",
    "\n",
    "# 연결 Client 종료\n",
    "client.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import \"experimental\"\n",
      "from(bucket: \"milesight\")\n",
      "  |> range(start: 2024-04-30T15:00:00Z, stop: 2024-05-01T15:00:00Z)\n",
      "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
      "  |> filter(fn: (r) => r[\"_field\"] == \"line_periodic_data_1_out\" or r[\"_field\"] == \"line_periodic_data_1_in\")\n",
      "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
      "  |> map(fn: (r) => ({r with _time: experimental.addDuration(d: 9h, to: r._time)}))\n",
      "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"_field\", \"device\"])\n",
      "\n",
      "                  time   value    place location     device\n",
      "0  2024-05-01 00:02:00  5012.5  class_a     main  gems-3500\n",
      "1  2024-05-01 00:04:00  4723.0  class_a     main  gems-3500\n",
      "                  time   value    place location     device\n",
      "0  2024-05-01 00:02:00  759.45  class_a     main  gems-3500\n",
      "1  2024-05-01 00:04:00  759.65  class_a     main  gems-3500\n",
      "                  time  value    place             location            device\n",
      "0  2024-05-01 00:02:00   23.1  class_a  bottom_right_corner  24e124126d152969\n",
      "1  2024-05-01 00:04:00   23.1  class_a  bottom_right_corner  24e124126d152969\n",
      "                  time  value    place location            device\n",
      "0  2024-05-01 00:02:00  546.5  class_a   indoor  24e124128c067999\n",
      "1  2024-05-01 00:04:00  544.5  class_a   indoor  24e124128c067999\n",
      "                  time  value    place location            device\n",
      "0  2024-05-01 00:02:00   69.0  class_a   indoor  24e124128c067999\n",
      "1  2024-05-01 00:04:00   69.0  class_a   indoor  24e124128c067999\n",
      "                  time  value    place             location            device\n",
      "0  2024-05-01 00:02:00   49.0  class_a  bottom_right_corner  24e124126d152969\n",
      "1  2024-05-01 00:04:00   48.5  class_a  bottom_right_corner  24e124126d152969\n",
      "                  time  value    place                 location device\n",
      "0  2024-05-01 01:00:00    0.0  class_a  line_periodic_data_1_in  vs133\n",
      "1  2024-05-01 02:00:00    0.0  class_a  line_periodic_data_1_in  vs133\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T02:27:37.611905Z",
     "start_time": "2024-05-02T02:27:36.591983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 내가 작성한 곳 \n",
    "\n",
    "# DB 연결 및 쿼리 실행\n",
    "client = create_client(url, token, org)\n",
    "\n",
    "# 전력(W) 조회(class a : main) Flux 쿼리\n",
    "query_power_device_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"powermetrics_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"phase\"] == \"total\")\n",
    "  |> filter(fn: (r) => r[\"description\"] == \"w\")\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"location\"] != \"main\" and r[\"location\"] != \"outdoor_unit_room_light\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 전력 CSV 생성\n",
    "df_power = query_to_dataframe(client, query_power_device_data)\n",
    "print(df_power.head(2))\n",
    "save_csv(df_power, \"%m_%d_power_device_data.csv\", \"all_data/power/device/\")\n",
    "\n",
    "# 전력 사용량 조회(class a : main) Flux 쿼리\n",
    "query_power_usage_device_data = f'''\n",
    "import \"experimental\"\n",
    "from(bucket: \"powermetrics_data\")\n",
    "  |> range(start: {start_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')}, stop: {end_time_utc.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "  |> filter(fn: (r) => r[\"phase\"] == \"kwh\")\n",
    "  |> filter(fn: (r) => r[\"place\"] == \"class_a\")\n",
    "  |> filter(fn: (r) => r[\"description\"] == \"sum\")\n",
    "  |> filter(fn: (r) => r[\"location\"] != \"main\" and r[\"location\"] != \"outdoor_unit_room_light\")\n",
    "  |> aggregateWindow(every: 2m, fn: mean, createEmpty: false)\n",
    "  |> map(fn: (r) => ({{r with _time: experimental.addDuration(d: 9h, to: r._time)}}))\n",
    "  |> keep(columns: [\"_time\", \"_value\", \"place\", \"location\", \"device\"])\n",
    "'''\n",
    "\n",
    "# 전력 사용량 CSV 생성\n",
    "df_power_usage = query_to_dataframe(client, query_power_usage_device_data)\n",
    "save_csv(df_power_usage, \"%m_%d_power_usage_device_data.csv\", \"all_data/power_usage/device/\")\n",
    "print(df_power_usage.head(2))\n",
    "\n",
    "# 연결 Client 종료\n",
    "client.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  time  value    place        location     device\n",
      "0  2024-05-01 00:02:00  200.0  class_a  ac_indoor_unit  gems-3500\n",
      "1  2024-05-01 00:04:00  199.0  class_a  ac_indoor_unit  gems-3500\n",
      "                  time  value    place        location     device\n",
      "0  2024-05-01 00:02:00   39.8  class_a  ac_indoor_unit  gems-3500\n",
      "1  2024-05-01 00:04:00   39.8  class_a  ac_indoor_unit  gems-3500\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T01:05:10.416314Z",
     "start_time": "2024-05-02T01:05:10.164174Z"
    }
   },
   "source": [
    "power_path = 'all_data/power/total'\n",
    "power_total_df = merge_data(power_path)\n",
    "print(power_total_df.head(2), \"\\n\")\n",
    "\n",
    "power_usage_path = 'all_data/power_usage/total'\n",
    "power_usage_total_df = merge_data(power_usage_path)\n",
    "print(power_usage_total_df.head(2), \"\\n\")\n",
    "\n",
    "temperature_path = 'all_data/temperature'\n",
    "temperature_total_df = merge_data(temperature_path)\n",
    "print(temperature_total_df.head(2), \"\\n\")\n",
    "\n",
    "co2_path = 'all_data/co2'\n",
    "co2_total_df = merge_data(co2_path)\n",
    "print(co2_total_df.head(2), \"\\n\")\n",
    "\n",
    "illumination_path = 'all_data/illumination'\n",
    "illumination_total_df = merge_data(illumination_path)\n",
    "print(illumination_total_df.head(2), \"\\n\")\n",
    "\n",
    "humidity_path = 'all_data/humidity'\n",
    "humidity_total_df = merge_data(humidity_path)\n",
    "print(humidity_total_df.head(2), \"\\n\")\n",
    "\n",
    "counter_path = 'all_data/counter/total'\n",
    "counter_total_df = merge_data(counter_path)\n",
    "print(counter_total_df.head(2), \"\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    time  value    place location\n",
      "9345 2024-04-15 00:02:00  152.0  class_a     main\n",
      "9346 2024-04-15 00:04:00  150.5  class_a     main \n",
      "\n",
      "                    time  value    place location\n",
      "5743 2024-04-15 00:02:00  264.7  class_a     main\n",
      "5744 2024-04-15 00:04:00  264.7  class_a     main \n",
      "\n",
      "                     time  value    place             location\n",
      "24282 2024-04-15 00:02:00   24.0  class_a  bottom_right_corner\n",
      "24986 2024-04-15 00:02:00   24.6  class_a               indoor \n",
      "\n",
      "                     time  value    place location\n",
      "10059 2024-04-15 00:02:00  658.0  class_a   indoor\n",
      "10060 2024-04-15 00:04:00  659.0  class_a   indoor \n",
      "\n",
      "                    time  value    place             location\n",
      "8618 2024-04-15 00:02:00   49.5  class_a  bottom_right_corner\n",
      "9322 2024-04-15 00:02:00   45.5  class_a               indoor \n",
      "\n",
      "                     time  value    place             location\n",
      "21416 2024-04-15 00:02:00   49.5  class_a  bottom_right_corner\n",
      "22120 2024-04-15 00:02:00   45.5  class_a               indoor \n",
      "\n",
      "                  time  value    place                  location\n",
      "0  2024-05-01 01:00:00    0.0  class_a   line_periodic_data_1_in\n",
      "23 2024-05-01 01:00:00    0.0  class_a  line_periodic_data_1_out \n",
      "\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T07:27:20.089779Z",
     "start_time": "2024-05-02T07:27:20.020859Z"
    }
   },
   "source": [
    "temperature_total_df = df_hourly_calculate(temperature_total_df, \"average_temperature(°C)\", \"mean\")\n",
    "humidity_total_df = df_hourly_calculate(humidity_total_df, \"average_humidity(%)\", \"mean\")\n",
    "co2_total_df = df_hourly_calculate(co2_total_df, \"average_co2(ppm)\", \"mean\")\n",
    "illumination_total_df = df_hourly_calculate(illumination_total_df, \"average_illumination(lux)\", \"mean\")\n",
    "counter_total_df = calculate_attendance(counter_total_df)"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T07:27:22.013239Z",
     "start_time": "2024-05-02T07:27:21.991061Z"
    }
   },
   "source": [
    "# 데이터프레임 병합\n",
    "total_environment_data_df = merge_dataframes([counter_total_df, temperature_total_df, humidity_total_df, co2_total_df, illumination_total_df])\n",
    "\n",
    "# 결과 확인\n",
    "print(total_environment_data_df)\n",
    "\n",
    "# 인덱스를 일반 컬럼으로 변경\n",
    "final_df1 = total_environment_data_df.reset_index()\n",
    "\n",
    "# CSV 파일로 저장\n",
    "final_df1.to_csv('all_data/class_a_environmental_sensor.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       in   out  cumulative  average_temperature(°C)  \\\n",
      "time                                                                   \n",
      "2024-04-15 00:00:00   NaN   NaN         NaN                23.918247   \n",
      "2024-04-15 01:00:00   NaN   NaN         NaN                23.331073   \n",
      "2024-04-15 02:00:00   NaN   NaN         NaN                23.240556   \n",
      "2024-04-15 03:00:00   NaN   NaN         NaN                23.148472   \n",
      "2024-04-15 04:00:00   NaN   NaN         NaN                22.955322   \n",
      "...                   ...   ...         ...                      ...   \n",
      "2024-05-01 20:00:00  11.0  11.0         0.0                23.611667   \n",
      "2024-05-01 21:00:00  15.0   9.0         6.0                23.312037   \n",
      "2024-05-01 22:00:00   9.0  10.0         5.0                23.214962   \n",
      "2024-05-01 23:00:00   0.0   0.0         5.0                23.116479   \n",
      "2024-05-02 00:00:00   0.0   0.0         5.0                23.016667   \n",
      "\n",
      "                     average_humidity(%)  average_co2(ppm)  \\\n",
      "time                                                         \n",
      "2024-04-15 00:00:00            47.913075        634.270115   \n",
      "2024-04-15 01:00:00            48.881356        611.783333   \n",
      "2024-04-15 02:00:00            48.548611        591.683333   \n",
      "2024-04-15 03:00:00            48.346528        581.433333   \n",
      "2024-04-15 04:00:00            48.401261        570.238889   \n",
      "...                                  ...               ...   \n",
      "2024-05-01 20:00:00            38.694444        619.944444   \n",
      "2024-05-01 21:00:00            39.969444        684.833333   \n",
      "2024-05-01 22:00:00            39.918561        676.661111   \n",
      "2024-05-01 23:00:00            39.639513        584.022222   \n",
      "2024-05-02 00:00:00            38.833333        541.000000   \n",
      "\n",
      "                     average_illumination(lux)  \n",
      "time                                            \n",
      "2024-04-15 00:00:00                  47.913075  \n",
      "2024-04-15 01:00:00                  48.881356  \n",
      "2024-04-15 02:00:00                  48.548611  \n",
      "2024-04-15 03:00:00                  48.346528  \n",
      "2024-04-15 04:00:00                  48.401261  \n",
      "...                                        ...  \n",
      "2024-05-01 20:00:00                  68.333333  \n",
      "2024-05-01 21:00:00                  68.433333  \n",
      "2024-05-01 22:00:00                  66.822222  \n",
      "2024-05-01 23:00:00                  65.777778  \n",
      "2024-05-02 00:00:00                  65.000000  \n",
      "\n",
      "[409 rows x 7 columns]\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T01:05:10.561050Z",
     "start_time": "2024-05-02T01:05:10.537306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 전체 전력/전력량\n",
    "power_total_df = df_hourly_calculate(power_total_df, \"total_power(Wh)\", \"sum\")\n",
    "power_usage_total_df = df_hourly_calculate(power_usage_total_df, 'total_power_usage(Kwh)', 'diff')\n",
    "\n",
    "# 실내기 전력/전력량\n",
    "\n",
    "# 실외기 전력/전력량\n",
    "\n",
    "# 자동문 전력/전력량\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T01:05:10.571686Z",
     "start_time": "2024-05-02T01:05:10.562412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_power_data_df = merge_dataframes([power_total_df, power_usage_total_df])\n",
    "print(total_power_data_df)\n",
    "final_df2 = total_power_data_df.reset_index()\n",
    "final_df2.to_csv('all_data/class_a_power_sensor.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     total_power(Wh)  total_power_usage(Kwh)\n",
      "time                                                        \n",
      "2024-04-15 00:00:00           2234.5                    0.20\n",
      "2024-04-15 01:00:00           6152.5                    0.20\n",
      "2024-04-15 02:00:00           5315.5                    0.20\n",
      "2024-04-15 03:00:00           3457.5                    0.10\n",
      "2024-04-15 04:00:00           6012.5                    0.20\n",
      "...                              ...                     ...\n",
      "2024-05-01 20:00:00          31287.5                    1.00\n",
      "2024-05-01 21:00:00          17475.5                    0.50\n",
      "2024-05-01 22:00:00          16100.5                    0.50\n",
      "2024-05-01 23:00:00          17601.5                    0.50\n",
      "2024-05-02 00:00:00            961.0                    0.05\n",
      "\n",
      "[409 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T06:55:16.850632Z",
     "start_time": "2024-05-02T06:55:16.832324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 환경 설정\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# CSV 파일을 로드하고 데이터프레임을 생성\n",
    "def load_and_process_data(directory, identifier):\n",
    "    frames = []\n",
    "    print(f\"디렉토리 검색: {directory}\")  # 디렉토리 출력\n",
    "    for file_name in os.listdir(directory):\n",
    "        print(f\"검사 중인 파일: {file_name}\")  # 검사 중인 파일 이름 출력\n",
    "        if identifier in file_name:\n",
    "            print(f\"로드 중인 파일: {file_name}\")  # 로드하는 파일 이름 출력\n",
    "            df = pd.read_csv(os.path.join(directory, file_name))\n",
    "            print(f\"로드된 데이터:\\n{df.head()}\")  # 로드된 데이터의 첫 5행 출력\n",
    "            df['time'] = pd.to_datetime(df['time'])\n",
    "            frames.append(df)\n",
    "    if frames:\n",
    "        combined_df = pd.concat(frames, ignore_index=True)\n",
    "        print(f\"병합된 데이터프레임 정보:\\n{combined_df.info()}\")  # 병합 후 데이터프레임 정보 출력\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"데이터가 없습니다.\")  # 데이터가 없는 경우 메시지 출력\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "# 각 장치 유형별 데이터 로드\n",
    "base_directory = 'all_data/power/device'\n",
    "ac_in_df = load_and_process_data(base_directory, 'ac_in')\n",
    "ac_out_df = load_and_process_data(base_directory, 'ac_out')\n",
    "auto_indoor_df = load_and_process_data(base_directory, 'auto_indoor')\n",
    "\n",
    "# 데이터 저장\n",
    "def save_data(df, filename):\n",
    "    directory = os.path.dirname(filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"데이터가 저장되었습니다: {filename}\")\n",
    "\n",
    "# 데이터 프레임 내용 출력\n",
    "print(\"AC Indoor DataFrame (a/c_in_df):\")\n",
    "print(ac_in_df.head())  # 첫 5개 행 출력\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(ac_in_df.info())  # 데이터 프레임 정보 출력\n",
    "\n",
    "print(\"\\nAC Outdoor DataFrame (a/c_in_df):\")\n",
    "print(ac_out_df.head())\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(ac_out_df.info())\n",
    "\n",
    "print(\"\\nAuto Indoor DataFrame (auto_indoor_unit):\")\n",
    "print(auto_indoor_df.head())\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(auto_indoor_df.info())\n",
    "\n",
    "# 데이터 저장\n",
    "print(\"데이터 저장 중..\")\n",
    "save_data(ac_in_df, 'path/to/save/ac_in.csv')\n",
    "save_data(ac_out_df, 'path/to/save/ac_out.csv')\n",
    "save_data(auto_indoor_df, 'path/to/save/auto_indoor.csv')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디렉토리 검색: all_data/power/device\n",
      "검사 중인 파일: 04_15_power_device_data.csv\n",
      "검사 중인 파일: 04_16_power_device_data.csv\n",
      "검사 중인 파일: 04_17_power_device_data.csv\n",
      "검사 중인 파일: 04_18_power_device_data.csv\n",
      "검사 중인 파일: 04_19_power_device_data.csv\n",
      "검사 중인 파일: 04_20_power_device_data.csv\n",
      "검사 중인 파일: 04_21_power_device_data.csv\n",
      "검사 중인 파일: 04_22_power_device_data.csv\n",
      "검사 중인 파일: 04_23_power_device_data.csv\n",
      "검사 중인 파일: 04_24_power_device_data.csv\n",
      "검사 중인 파일: 04_25_power_device_data.csv\n",
      "검사 중인 파일: 04_26_power_device_data.csv\n",
      "검사 중인 파일: 04_27_power_device_data.csv\n",
      "검사 중인 파일: 04_28_power_device_data.csv\n",
      "검사 중인 파일: 04_29_power_device_data.csv\n",
      "검사 중인 파일: 04_30_power_device_data.csv\n",
      "검사 중인 파일: 05_01_power_device_data.csv\n",
      "데이터가 없습니다.\n",
      "디렉토리 검색: all_data/power/device\n",
      "검사 중인 파일: 04_15_power_device_data.csv\n",
      "검사 중인 파일: 04_16_power_device_data.csv\n",
      "검사 중인 파일: 04_17_power_device_data.csv\n",
      "검사 중인 파일: 04_18_power_device_data.csv\n",
      "검사 중인 파일: 04_19_power_device_data.csv\n",
      "검사 중인 파일: 04_20_power_device_data.csv\n",
      "검사 중인 파일: 04_21_power_device_data.csv\n",
      "검사 중인 파일: 04_22_power_device_data.csv\n",
      "검사 중인 파일: 04_23_power_device_data.csv\n",
      "검사 중인 파일: 04_24_power_device_data.csv\n",
      "검사 중인 파일: 04_25_power_device_data.csv\n",
      "검사 중인 파일: 04_26_power_device_data.csv\n",
      "검사 중인 파일: 04_27_power_device_data.csv\n",
      "검사 중인 파일: 04_28_power_device_data.csv\n",
      "검사 중인 파일: 04_29_power_device_data.csv\n",
      "검사 중인 파일: 04_30_power_device_data.csv\n",
      "검사 중인 파일: 05_01_power_device_data.csv\n",
      "데이터가 없습니다.\n",
      "디렉토리 검색: all_data/power/device\n",
      "검사 중인 파일: 04_15_power_device_data.csv\n",
      "검사 중인 파일: 04_16_power_device_data.csv\n",
      "검사 중인 파일: 04_17_power_device_data.csv\n",
      "검사 중인 파일: 04_18_power_device_data.csv\n",
      "검사 중인 파일: 04_19_power_device_data.csv\n",
      "검사 중인 파일: 04_20_power_device_data.csv\n",
      "검사 중인 파일: 04_21_power_device_data.csv\n",
      "검사 중인 파일: 04_22_power_device_data.csv\n",
      "검사 중인 파일: 04_23_power_device_data.csv\n",
      "검사 중인 파일: 04_24_power_device_data.csv\n",
      "검사 중인 파일: 04_25_power_device_data.csv\n",
      "검사 중인 파일: 04_26_power_device_data.csv\n",
      "검사 중인 파일: 04_27_power_device_data.csv\n",
      "검사 중인 파일: 04_28_power_device_data.csv\n",
      "검사 중인 파일: 04_29_power_device_data.csv\n",
      "검사 중인 파일: 04_30_power_device_data.csv\n",
      "검사 중인 파일: 05_01_power_device_data.csv\n",
      "데이터가 없습니다.\n",
      "AC Indoor DataFrame (a/c_in_df):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n",
      "None\n",
      "\n",
      "AC Outdoor DataFrame (a/c_in_df):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n",
      "None\n",
      "\n",
      "Auto Indoor DataFrame (auto_indoor_unit):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n",
      "None\n",
      "데이터 저장 중..\n",
      "데이터가 저장되었습니다: path/to/save/ac_in.csv\n",
      "데이터가 저장되었습니다: path/to/save/ac_out.csv\n",
      "데이터가 저장되었습니다: path/to/save/auto_indoor.csv\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T06:44:57.154540Z",
     "start_time": "2024-05-02T06:44:57.142911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_data(directory, identifier):\n",
    "    combined_df = pd.DataFrame()  # 초기 빈 데이터프레임 생성\n",
    "    for file_name in os.listdir(directory):\n",
    "        if identifier in file_name:  # 식별자를 포함하는 파일 이름 확인\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# 기본 디렉토리 설정\n",
    "base_directory = 'all_data/power/device'  # 경로는 실제 환경에 맞게 조정해주세요.\n",
    "\n",
    "# 각 장치 유형별 데이터 로드\n",
    "a_c_in_df = load_data(base_directory, 'ac_in')\n",
    "a_c_out_df = load_data(base_directory, 'ac_out')\n",
    "auto_indoor_df = load_data(base_directory, 'auto_indoor')\n",
    "\n",
    "print(\"A/C Indoor Unit Data:\")\n",
    "print(a_c_in_df.head())\n",
    "\n",
    "print(\"\\nA/C Outdoor Unit Data:\")\n",
    "print(a_c_out_df.head())\n",
    "\n",
    "print(\"\\nAuto Indoor Unit Data:\")\n",
    "print(auto_indoor_df.head())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A/C Indoor Unit Data:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "A/C Outdoor Unit Data:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Auto Indoor Unit Data:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T07:25:07.422230Z",
     "start_time": "2024-05-02T07:25:07.095005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv 파일에서 데이터 로드 하여 데이터프레임으로 반환\n",
    "def load_data(filepath):\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "# 전체 데이터셋 로드\n",
    "power_data_path = '/path/to/04_16_power_device_data.csv'  # 이 경로를 업데이트하세요\n",
    "power_data = load_data(power_data_path)\n",
    "\n",
    "# 각 장치 유형별로 데이터를 필터링하여 데이터프레임 생성\n",
    "ac_in_df = power_data[power_data['location'].str.contains('ac_in', case=False, na=False)]\n",
    "ac_out_df = power_data[power_data['location'].str.contains('ac_out', case=False, na=False)]\n",
    "auto_indoor_df = power_data[power_data['location'].str.contains('auto_indoor', case=False, na=False)]\n",
    "\n",
    "# 각 데이터프레임의 첫 몇 줄을 출력하여 확인\n",
    "print(\"AC 인도어 데이터프레임:\")\n",
    "print(ac_in_df.head(), \"\\n\")\n",
    "\n",
    "print(\"AC 아웃도어 데이터프레임:\")\n",
    "print(ac_out_df.head(), \"\\n\")\n",
    "\n",
    "print(\"자동 인도어 데이터프레임:\")\n",
    "print(auto_indoor_df.head(), \"\\n\")"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/04_16_power_device_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# 전체 데이터셋 로드\u001B[39;00m\n\u001B[0;32m      8\u001B[0m power_data_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/path/to/04_16_power_device_data.csv\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# 이 경로를 업데이트하세요\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m power_data \u001B[38;5;241m=\u001B[39m load_data(power_data_path)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# 각 장치 유형별로 데이터를 필터링하여 데이터프레임 생성\u001B[39;00m\n\u001B[0;32m     12\u001B[0m ac_in_df \u001B[38;5;241m=\u001B[39m power_data[power_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mcontains(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mac_in\u001B[39m\u001B[38;5;124m'\u001B[39m, case\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, na\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)]\n",
      "Cell \u001B[1;32mIn[20], line 5\u001B[0m, in \u001B[0;36mload_data\u001B[1;34m(filepath)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_data\u001B[39m(filepath):\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mread_csv(filepath)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    945\u001B[0m )\n\u001B[0;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[0;32m   1706\u001B[0m     f,\n\u001B[0;32m   1707\u001B[0m     mode,\n\u001B[0;32m   1708\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m   1709\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m   1710\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[0;32m   1711\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[0;32m   1712\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1713\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m   1714\u001B[0m )\n\u001B[0;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    864\u001B[0m             handle,\n\u001B[0;32m    865\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m    866\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[0;32m    867\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[0;32m    868\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    869\u001B[0m         )\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/path/to/04_16_power_device_data.csv'"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoothing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
